---
title: "Homework 5"
author: "Kairui Wang"
date: "2023-11-05"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

Load key packages.

```{r,message = FALSE}
library(tidyverse)
library(rvest)
library(broom)
library(plotly)
library(dplyr)
```

Set seed for reproducibility.

```{r}
set.seed(12345)
```


## Problem 1

Read the dataset from the corrected URL

```{r}
data_url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data <- read_csv(data_url)
```

Describe the raw data:

```{r}
glimpse(homicide_data)
head(homicide_data)
```

Create a city_state variable and summarize the data within cities

```{r}
sum_homicide_data <- 
  homicide_data |> 
  mutate(city_state = paste(city, state, sep = ", ")) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) |> 
  ungroup()

print(sum_homicide_data)
```

Filter the data for Baltimore, MD

```{r}
baltimore_data <- 
  homicide_data |> 
  filter(city == "Baltimore" & state == "MD")
```

Use prop.test to estimate the proportion of unsolved homicides

```{r}
prop_test_result <- 
  prop.test(
  sum(baltimore_data$disposition %in% c("Closed without arrest", "Open/No arrest")),
  nrow(baltimore_data)
)
```

Apply broom::tidy to the prop.test result

```{r}
baltimore_tidy <- 
  tidy(prop_test_result)
```

Extract the estimated proportion and confidence intervals

```{r}
baltimore_proportion <- baltimore_tidy$p.value
baltimore_conf_interval <- prop_test_result$conf.int
```

Print the results

```{r}
print(baltimore_tidy)
```

Create a list column for each city and apply prop.test

```{r}
result_df <- 
  homicide_data |> 
  group_by(city) |> 
  nest() |> 
  mutate(
    prop_test_result = map(data, ~ prop.test(
      sum(.x$disposition %in% c("Closed without arrest", "Open/No arrest")),
      nrow(.x)
    )),
    tidy_results = map(prop_test_result, broom::tidy)
  ) |> 
  unnest(tidy_results) |> 
  select(city, estimate, conf.low, conf.high)

print(result_df)

```

Create a plot

```{r}
result_df |> 
  mutate(city = fct_reorder(city, estimate)) |> 
  ggplot(aes(x = estimate, y = city)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  labs(
    x = "Proportion of Unsolved Homicides",
    y = "City",
    title = "Proportion of Unsolved Homicides by City",
    subtitle = "Estimates and Confidence Intervals"
  ) +
   theme_minimal()

```


## Problem 2

List all files in the extracted folder

```{r}
file_names <- list.files(path = "data", pattern = ".csv", full.names = TRUE)
```

Read in data for each subject using purrr::map

```{r}
data_list <- map(file_names, ~ read_csv(.x) %>% mutate(file_id = basename(.x)))
```

Tidy the result

```{r}
tidy_data <- bind_rows(data_list) %>%
  separate(file_id, into = c("arm", "subject_id"), sep = "_") %>%
  mutate(group = ifelse(grepl("con", file_names), "Control", "Experimental")) %>%
  mutate(subject_ID = as.integer(str_extract(file_names, "[0-9][0-9]"))) %>%
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    values_to = "observation"
  ) %>%
  mutate(week = as.numeric(gsub("week_", "", week)))

```

Create a spaghetti plot

```{r}
tidy_data %>% 
  ggplot(aes(x = week, y = observation, color = as.factor(subject_ID))) +
  geom_point() +
  geom_line(aes(group = subject_ID), alpha = 0.5) +
  facet_grid(~group) +
  labs(x = "Week", y = "Observation", col = "Subject ID")
```

* Compared with the control group, the observed values of the experimental group showed a significant upward trend over time.

* There were slight differences in the results observed within the groups, but the lines of observation generally followed a similar pattern for most subjects.

* Comparing the line trajectories of the control group and the experimental group, there was no significant difference in the rate of change of the observed values over time.


## Problem 3

Set design elements
```{r}
n <- 30
sigma <- 5
alpha <- 0.05
true_values <- 0:6
num_simulations <- 5000
```

Initialize data frames to store results

```{r}
power_results <- data.frame(mu = true_values, power = numeric(length(true_values)))
average_estimate_results <- data.frame(mu = true_values, 
                                       avg_estimate = numeric(length(true_values)),
                                       avg_estimate_rejected = numeric(length(true_values)))
```

Perform simulations for different values of μ

```{r}
for (mu in true_values) {
  rejected_count <- 0
  total_estimate <- 0
  total_estimate_rejected <- 0
  
  for (i in 1:num_simulations) {
    # Generate a random sample from Normal distribution
    sample_data <- rnorm(n, mean = mu, sd = sigma)
    
    # Perform one-sample t-test
    t_test_result <- t.test(sample_data, mu = 0)
    
    # Use broom::tidy to clean the t.test result
    tidy_result <- tidy(t_test_result)
    
    # Save the estimate and p-value
    estimate <- tidy_result$estimate
    p_value <- tidy_result$p.value
    
    # Check if null hypothesis is rejected
    if (p_value < alpha) {
      rejected_count <- rejected_count + 1
      total_estimate_rejected <- total_estimate_rejected + estimate
    }
    
    total_estimate <- total_estimate + estimate
  }
  
  # Calculate power
  power <- rejected_count / num_simulations
  
  # Calculate average estimate
  avg_estimate <- total_estimate / num_simulations
  
  # Calculate average estimate for rejected cases
  avg_estimate_rejected <- total_estimate_rejected / rejected_count
  
  # Store results in data frames
  power_results$power[power_results$mu == mu] <- power
  average_estimate_results$avg_estimate[average_estimate_results$mu == mu] <- avg_estimate
  average_estimate_results$avg_estimate_rejected[average_estimate_results$mu == mu] <- avg_estimate_rejected
}
```

Plot the proportion of times the null was rejected (power)

```{r}
ggplot(power_results, aes(x = mu, y = power)) +
  geom_point() +
  geom_line() + 
  labs(
    x = "True Value of μ",
    y = "Power (Proportion of Rejections)",
    title = "Power vs. Effect Size"
  )
```

Plot the average estimate of μ̂

```{r}
ggplot(average_estimate_results, aes(x = mu, y = avg_estimate, group = 1)) +
  geom_point(aes(color = "steelblue"), size = 3) +
  geom_line(aes(y = avg_estimate, color = "steelblue"), size = 1) +
  geom_point(data = average_estimate_results, aes(x = mu, y = avg_estimate_rejected, color = "tomato"), size = 3) +
  geom_line(data = average_estimate_results, aes(x = mu, y = avg_estimate_rejected, color = "tomato"), size = 1) +
  labs(
    x = "True Value of μ",
    y = "Average Estimate of μ̂",
    title = "Average Estimate of μ̂ vs. Effect Size"
  ) +
  scale_color_identity() +
  scale_x_continuous(limits = c(1, max(average_estimate_results$mu))) +
  scale_y_continuous(limits = c(1, max(average_estimate_results$avg_estimate, average_estimate_results$avg_estimate_rejected)))
```

* When the effect size is less than 4 (in this case), when the null hypothesis is rejected, the μ̂ (sample average) differs from the truth value of mu, and the T-value (a statistic) is always greater than the truth value of mu. This is because the effect size is relatively small and the efficacy is relatively low.

* When the effect size is greater than or equal to 4 (in this case), the sample average of μ̂ is roughly equal to the true value of mu when the null hypothesis is rejected. This is because as the effect size increases, so does the efficacy.
